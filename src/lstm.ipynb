{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a9d40e3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8369efc5",
   "metadata": {},
   "source": [
    "Attention class (https://www.geeksforgeeks.org/nlp/adding-attention-layer-to-a-bi-lstm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "576312d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionLayer(nn.Module):\n",
    "    def __init__(self, input_dim, attention_dim):\n",
    "        super(AttentionLayer, self).__init__()\n",
    "        self.attention =  nn.Sequential(\n",
    "            nn.Linear(input_dim, attention_dim),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(attention_dim, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        # inputs: (batch_size, seq_len, input_dim)\n",
    "        scores = self.attention(inputs)  # (batch_size, seq_len, 1)\n",
    "        weights = torch.sigmoid(scores)  # (batch_size, seq_len, 1) apply sigmoid to get weights between 0 and 1\n",
    "        \n",
    "        # return the weighted sequence\n",
    "        weighted_seq = inputs * weights # (batch_size, seq_len, input_dim), if we need a single value per sequence, we can sum over seq_len dimension\n",
    "        return weighted_seq, weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fa602a0",
   "metadata": {},
   "source": [
    "Bidirectional LSTM class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "558e539d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BLSTMWithAttention(nn.Module):\n",
    "    def __init__(self, \n",
    "                 input_dim,                         # bert embedding dim\n",
    "                 hidden_dim,                        # lstm hidden dim\n",
    "                 output_dim,                        # number of classes\n",
    "                 num_layers=1,                      # lstm layers\n",
    "                 bidirectional=True,                # bidirectional lstm\n",
    "                 dropout=0.5,                       # dropout rate\n",
    "                 use_attention=True,                # use attention mechanism\n",
    "                 attention_dim=128,                  # attention layer dimension\n",
    "                 autoregress = False                  # autoregressive flag  \n",
    "                 ):\n",
    "        super(BLSTMWithAttention, self).__init__()\n",
    "        self.use_attention = use_attention\n",
    "        self.autoregress = autoregress\n",
    "\n",
    "        input_size = input_dim + 2 if autoregress else input_dim\n",
    "        self.blstm = nn.LSTM(input_size=input_size, \n",
    "                             hidden_size=hidden_dim, \n",
    "                             num_layers=num_layers, \n",
    "                             batch_first=True,\n",
    "                             bidirectional=bidirectional)\n",
    "        \n",
    "        if self.use_attention:\n",
    "            final_input_dim = hidden_dim * 2 if bidirectional else hidden_dim\n",
    "            self.attention_layer = AttentionLayer(final_input_dim, attention_dim)\n",
    "        \n",
    "        self.fc = nn.Linear(hidden_dim * 2 if bidirectional else hidden_dim, output_dim)\n",
    "        self.dropout = nn.Dropout(dropout)  \n",
    "    \n",
    "    def forward(self, x, past_status=None, hidden=None):\n",
    "        if self.autoregress :\n",
    "            if past_status is None:\n",
    "                raise ValueError(\"past_status must be provided for autoregressive mode\")\n",
    "            \n",
    "            # concat the past status with current input\n",
    "            x = torch.cat((past_status, x), dim=2)  # assuming past_status shape is (batch_size, past_seq_len, 2)\n",
    "            out, next_state = self.blstm(x, hidden)\n",
    "        else:\n",
    "            out, weight = self.blstm(x)\n",
    "        \n",
    "        if self.use_attention:\n",
    "            out, _ = self.attention_layer(out)\n",
    "        out = self.dropout(out)\n",
    "        prediction = self.fc(out)\n",
    "        \n",
    "        return prediction, next_state, weight if self.use_attention else None\n",
    "        \n",
    "        \n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
