{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Mount Google Drive (for saving models)\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m-e9CPk8uQdf",
        "outputId": "0aa7663a-6460-47f2-bde4-6cef72b9542d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/AndreaLolli2912/SemEval2026-EmoVA.git\n",
        "%cd SemEval2026-EmoVA"
      ],
      "metadata": {
        "id": "7tFlzM4xuild",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d8068dcd-3806-4c79-d146-ce2808975caa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'SemEval2026-EmoVA'...\n",
            "remote: Enumerating objects: 495, done.\u001b[K\n",
            "remote: Counting objects: 100% (174/174), done.\u001b[K\n",
            "remote: Compressing objects: 100% (147/147), done.\u001b[K\n",
            "remote: Total 495 (delta 102), reused 72 (delta 27), pack-reused 321 (from 1)\u001b[K\n",
            "Receiving objects: 100% (495/495), 1.62 MiB | 5.32 MiB/s, done.\n",
            "Resolving deltas: 100% (262/262), done.\n",
            "/content/SemEval2026-EmoVA\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "import json\n",
        "import shutil\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from pathlib import Path\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.optim import AdamW\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
        "import types\n",
        "from scipy.stats import pearsonr\n",
        "\n",
        "# Import your modules\n",
        "from src.data.dataset import EmoVADataset\n",
        "from src.data.collate import create_collate_fn\n",
        "from src.models import AffectModel\n",
        "from src.models.tokenizer_wrapper import TokenizerWrapper\n",
        "from src.training import train_epoch, GradientClipper"
      ],
      "metadata": {
        "id": "bt9AI8MuSjiB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pwd"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_ii7Ea-E_SSm",
        "outputId": "919de69b-57e4-4f1e-bebd-02dc62287b32"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/SemEval2026-EmoVA\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "run_folder = '/content/drive/MyDrive/SEMEVAL2026_EMOVA/model_checkpoints/20260128_153120_score0.5920'\n",
        "\n",
        "print(f\"Loading checkpoint from: {checkpoint_path}\")\n",
        "checkpoint = torch.load(checkpoint_path, map_location='cpu')\n",
        "\n",
        "config_data = checkpoint['config']\n",
        "if isinstance(config_data, dict):\n",
        "    config = SimpleNamespace(**config_data)\n",
        "else:\n",
        "    config = config_data\n",
        "\n",
        "print(f\"Config loaded. Model: {config.model_name}, LoRA: {getattr(config, 'lora', False)}\")\n",
        "\n",
        "'''config_path = f\"{run_folder}/config.json\"\n",
        "print(f\"Loading config from: {config_path}\")\n",
        "\n",
        "with open(config_path, 'r') as f:\n",
        "    config_dict = json.load(f)\n",
        "\n",
        "# Convert dict to Namespace\n",
        "config = types.SimpleNamespace(**config_dict)'''"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g9PYk_T8Sf26",
        "outputId": "dcbc6420-70fe-4da1-9b12-2bc8fffa1709"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading config from: /content/drive/MyDrive/SEMEVAL2026_EMOVA/model_checkpoints/20260128_153120_score0.5920/config.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5y3mBMJYrjbo"
      },
      "outputs": [],
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "if hasattr(config, 'seed'):\n",
        "    torch.manual_seed(config.seed)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed_all(config.seed)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = TokenizerWrapper(config.model_name, config.max_text_length)\n",
        "full_dataset = EmoVADataset(config.data_path)\n",
        "collate_fn = create_collate_fn(tokenizer)\n",
        "train_loader = DataLoader(\n",
        "    full_dataset,\n",
        "    batch_size=config.batch_size,\n",
        "    shuffle=True,\n",
        "    collate_fn=collate_fn,\n",
        "    num_workers=config.num_workers\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 367
        },
        "id": "ZNWJ3cDdqyso",
        "outputId": "ee67d531-37d8-421b-f69a-1d9686c2ce01"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "'<' not supported between instances of 'str' and 'int'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1310367564.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mfull_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mEmoVADataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mcollate_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_collate_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m train_loader = DataLoader(\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mfull_dataset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, dataset, batch_size, shuffle, sampler, batch_sampler, num_workers, collate_fn, pin_memory, drop_last, timeout, worker_init_fn, multiprocessing_context, generator, prefetch_factor, persistent_workers, pin_memory_device, in_order)\u001b[0m\n\u001b[1;32m    263\u001b[0m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_log_api_usage_once\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"python.data_loader\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    264\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 265\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mnum_workers\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    266\u001b[0m             raise ValueError(\n\u001b[1;32m    267\u001b[0m                 \u001b[0;34m\"num_workers option should be non-negative; \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: '<' not supported between instances of 'str' and 'int'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''model = AffectModel(\n",
        "    model_path=config.model_name,\n",
        "    encoder_bitfit=config.encoder_bitfit,\n",
        "    isab_inducing_points=config.isab_inducing_points,\n",
        "    pma_num_seeds=config.pma_num_seeds,\n",
        "    lstm_hidden_dim=config.lstm_hidden_dim,\n",
        "    lstm_num_layers=config.lstm_num_layers,\n",
        "    lstm_bidirectional=True,\n",
        "    dropout=config.dropout,\n",
        "    constrain_output=False,\n",
        ")\n",
        "\n",
        "if config.encoder_bitfit:\n",
        "    model.encoder.backbone.gradient_checkpointing_enable()\n",
        "\n",
        "model = model.to(device)'''"
      ],
      "metadata": {
        "id": "obSnpLhirO78"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from src.models import AffectModel\n",
        "from peft import LoraConfig, get_peft_model, TaskType\n",
        "\n",
        "# 1. Inizializza Modello Base\n",
        "print(f\"Initializing base model: {config.model_name}\")\n",
        "model = AffectModel(\n",
        "    model_path=config.model_name,\n",
        "    encoder_bitfit=getattr(config, 'encoder_bitfit', False),\n",
        "    pma_num_seeds=config.pma_num_seeds,\n",
        "    isab_inducing_points=config.isab_inducing_points,\n",
        "    n_heads=config.n_heads,\n",
        "    lstm_hidden_dim=config.lstm_hidden_dim,\n",
        "    lstm_num_layers=config.lstm_num_layers,\n",
        "    lstm_bidirectional=True,\n",
        "    dropout=0.0, # Zero dropout for inference\n",
        "    constrain_output=getattr(config, 'constrain_output', False),\n",
        ")\n",
        "\n",
        "# 2. Applica LoRA (Se usato nel training)\n",
        "use_lora = getattr(config, 'lora', False)\n",
        "if use_lora:\n",
        "    print(\"Applying LoRA structure for weight loading...\")\n",
        "    peft_config = LoraConfig(\n",
        "        task_type=TaskType.FEATURE_EXTRACTION,\n",
        "        inference_mode=True,\n",
        "        r=8, lora_alpha=32, lora_dropout=0.1, target_modules=[\"query\", \"value\"]\n",
        "    )\n",
        "    # Tentativo di applicazione specifico (come in training) o globale\n",
        "    try:\n",
        "        model.encoder.backbone = get_peft_model(model.encoder.backbone, peft_config)\n",
        "    except Exception:\n",
        "        model = get_peft_model(model, peft_config)\n",
        "\n",
        "# 3. Carica i Pesi\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model.to(device)\n",
        "\n",
        "print(\"Loading state dict...\")\n",
        "try:\n",
        "    # strict=False Ã¨ fondamentale per LoRA salvati custom\n",
        "    keys = model.load_state_dict(checkpoint['model_state_dict'], strict=False)\n",
        "    print(f\"Loaded. Missing keys: {len(keys.missing_keys)} (Expected if base model keys are skipped)\")\n",
        "except Exception as e:\n",
        "    print(f\"Error loading weights: {e}\")\n",
        "\n",
        "model.eval()\n",
        "print(\"Model ready!\")"
      ],
      "metadata": {
        "id": "kMnZi_10RW4f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Optimizer with separate LRs\n",
        "param_groups = [\n",
        "    {'params': list([p for n, p in model.encoder.named_parameters() if p.requires_grad]),\n",
        "     'lr': 5e-6, 'name': 'encoder_bias'},\n",
        "    {'params': list(model.isab.parameters()), 'lr': config.lr, 'name': 'isab'} if model.isab else None,\n",
        "    {'params': list(model.pma.parameters()), 'lr': config.lr, 'name': 'pma'},\n",
        "    {'params': list(model.lstm.parameters()), 'lr': config.lr, 'name': 'lstm'},\n",
        "    {'params': list(model.head.parameters()), 'lr': config.lr, 'name': 'head'},\n",
        "]\n",
        "\n",
        "param_groups = [\n",
        "    pg for pg in param_groups\n",
        "    if pg is not None and len(pg['params']) > 0\n",
        "]\n",
        "\n",
        "optimizer = AdamW(param_groups, weight_decay=config.weight_decay)\n",
        "scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=5)\n",
        "clipper = GradientClipper(max_norm=config.max_grad_norm)\n",
        "\n",
        "for pg in optimizer.param_groups:\n",
        "    n_params = sum(p.numel() for p in pg['params'])\n",
        "    print(f\"{pg.get('name', 'unnamed')}: {n_params:,} params, lr={pg['lr']:.1e}\")"
      ],
      "metadata": {
        "id": "lQq0ssZbrbPp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(config.epochs):\n",
        "    # train_epoch returns dict with 'loss'\n",
        "    result = train_epoch(\n",
        "        model, train_loader, config.loss, optimizer, device, config, clipper=clipper\n",
        "    )\n",
        "    print(f\"   Epoch {epoch+1}/{config.epochs} | Loss: {result['loss']:.4f}\")"
      ],
      "metadata": {
        "id": "1zxMaMqcrxAu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "\n",
        "predictions = {}\n",
        "gold = {}\n",
        "\n",
        "with torch.no_grad():\n",
        "    for batch in tqdm(train_loader, desc=\"Inferencing\"):\n",
        "        input_ids = batch['input_ids'].to(device)\n",
        "        attention_mask = batch['attention_mask'].to(device)\n",
        "        seq_lengths = batch['seq_lengths'].to(device)\n",
        "        seq_mask = batch['seq_attention_mask'].to(device)\n",
        "\n",
        "        # Targets\n",
        "        valences = batch['valences'].to(device)\n",
        "        arousals = batch['arousals'].to(device)\n",
        "        user_ids = batch['user_ids']\n",
        "\n",
        "        # Forward\n",
        "        preds = model(input_ids, attention_mask, seq_lengths, seq_mask)\n",
        "\n",
        "        # Move to CPU\n",
        "        preds = preds.cpu().numpy()\n",
        "        valences = valences.cpu().numpy()\n",
        "        arousals = arousals.cpu().numpy()\n",
        "\n",
        "        # Organize by User\n",
        "        for i, uid in enumerate(user_ids):\n",
        "            valid_len = seq_lengths[i].item()\n",
        "\n",
        "            # Slice valid sequence\n",
        "            p_seq = preds[i, :valid_len, :]\n",
        "            t_val = valences[i, :valid_len]\n",
        "            t_aro = arousals[i, :valid_len]\n",
        "\n",
        "            # Stack targets [Seq, 2]\n",
        "            t_seq = np.stack([t_val, t_aro], axis=-1)\n",
        "\n",
        "            if uid not in predictions:\n",
        "                predictions[uid] = []\n",
        "                gold[uid] = []\n",
        "\n",
        "            predictions[uid].append(p_seq)\n",
        "            gold[uid].append(t_seq)\n",
        "\n",
        "# Concatenate if users were split across batches (rare with shuffle=True but possible)\n",
        "for uid in predictions:\n",
        "    predictions[uid] = np.concatenate(predictions[uid], axis=0)\n",
        "    gold[uid] = np.concatenate(gold[uid], axis=0)"
      ],
      "metadata": {
        "id": "59nDjAUPt5Lt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "user_pred_v = np.array([predictions[u][:, 0].mean() for u in predictions.keys()])\n",
        "user_pred_a = np.array([predictions[u][:, 1].mean() for u in predictions.keys()])\n",
        "user_gold_v = np.array([gold[u][:, 0].mean() for u in gold.keys()])\n",
        "user_gold_a = np.array([gold[u][:, 1].mean() for u in gold.keys()])\n",
        "\n",
        "r_val, _ = pearsonr(user_gold_v, user_pred_v)\n",
        "r_aro, _ = pearsonr(user_gold_a, user_pred_a)\n",
        "\n",
        "results = {\n",
        "    'valence/r_between': r_val,\n",
        "    'arousal/r_between': r_aro\n",
        "}\n",
        "\n",
        "print(f\"Training Fit (Between-User): Valence={r_val:.3f}, Arousal={r_aro:.3f}\")"
      ],
      "metadata": {
        "id": "8JmB1X0IuCvj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
        "\n",
        "# Valence\n",
        "axes[0].scatter(user_gold_v, user_pred_v, alpha=0.5, s=15, c='blue')\n",
        "axes[0].plot([-2, 2], [-2, 2], 'r--', label='Perfect', linewidth=2)\n",
        "axes[0].set_xlabel('Gold Valence (User Mean)')\n",
        "axes[0].set_ylabel('Predicted Valence (User Mean)')\n",
        "axes[0].set_title(f\"Valence - Between User (r={results['valence/r_between']:.3f})\")\n",
        "axes[0].legend()\n",
        "axes[0].grid(True, alpha=0.3)\n",
        "\n",
        "\n",
        "# Arousal\n",
        "axes[1].scatter(user_gold_a, user_pred_a, alpha=0.5, s=15, c='green')\n",
        "axes[1].plot([0, 2], [0, 2], 'r--', label='Perfect', linewidth=2)\n",
        "axes[1].set_xlabel('Gold Arousal (User Mean)')\n",
        "axes[1].set_ylabel('Predicted Arousal (User Mean)')\n",
        "axes[1].set_title(f\"Arousal - Between User (r={results['arousal/r_between']:.3f})\")\n",
        "axes[1].legend()\n",
        "axes[1].grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "vZ1lCfPuuODr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load test data\n",
        "test_path = '/content/drive/MyDrive/SEMEVAL2026_EMOVA/dataset/TEST_RELEASE_5JAN2026/test_subtask1.csv'\n",
        "test_df = pd.read_csv(test_path)\n",
        "\n",
        "# Sort by user and timestamp (same as training)\n",
        "test_df['timestamp'] = pd.to_datetime(test_df['timestamp'])\n",
        "test_df = test_df.sort_values(['user_id', 'timestamp']).reset_index(drop=True)\n",
        "\n",
        "print(f\"Test set: {len(test_df)} texts, {test_df['user_id'].nunique()} users\")"
      ],
      "metadata": {
        "id": "4RFbzTb0x2K5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "results_list = []\n",
        "\n",
        "results_list = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    # Use your preferred groupby loop\n",
        "    for user_id, group in tqdm(test_df.groupby('user_id', sort=False), desc=\"Predicting\"):\n",
        "        texts = group['text'].tolist()\n",
        "        text_ids = group['text_id'].tolist()\n",
        "\n",
        "        # Tokenize\n",
        "        tokenized = tokenizer(texts)\n",
        "        input_ids = tokenized['input_ids'].unsqueeze(0).to(device)  # [1, S, T]\n",
        "        attention_mask = tokenized['attention_mask'].unsqueeze(0).to(device)\n",
        "        seq_lengths = torch.tensor([len(texts)], device=device)\n",
        "        seq_mask = torch.ones(1, len(texts), device=device)\n",
        "\n",
        "        # Predict\n",
        "        preds = model(input_ids, attention_mask, seq_lengths, seq_mask)  # [1, S, 2]\n",
        "        preds = preds[0].cpu().numpy()  # [S, 2]\n",
        "\n",
        "        # Store results\n",
        "        for i, text_id in enumerate(text_ids):\n",
        "            results_list.append({\n",
        "                'user_id': user_id,\n",
        "                'text_id': text_id,\n",
        "                'valence': preds[i, 0],\n",
        "                'arousal': preds[i, 1],\n",
        "            })\n",
        "\n",
        "# --- 3. SAVE & ZIP ---\n",
        "submission_df = pd.DataFrame(results_list)\n",
        "\n",
        "# Verify\n",
        "print(\"\\nPreview:\")\n",
        "print(submission_df.head())\n",
        "\n",
        "# Save\n",
        "os.chdir('/content')\n",
        "submission_df.to_csv('pred_subtask1.csv', index=False)\n",
        "\n",
        "# Create Zip\n",
        "!zip -j submission.zip pred_subtask1.csv\n",
        "!unzip -l submission.zip"
      ],
      "metadata": {
        "id": "MNTP_E75x4dJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "os.chdir('/content')  # or /kaggle/working\n",
        "\n",
        "submission_df.to_csv('pred_subtask1.csv', index=False)\n",
        "!zip -j submission.zip pred_subtask1.csv\n",
        "\n",
        "!unzip -l submission.zip"
      ],
      "metadata": {
        "id": "ZUGscxEcx79m"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}